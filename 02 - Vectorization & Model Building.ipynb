{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9636d072",
   "metadata": {},
   "source": [
    "## Telco Customer Churn Prediction\n",
    "### Part 2: Vectorization & Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa324d25",
   "metadata": {},
   "source": [
    "Lets vectorize all the features to build the model<br>\n",
    "\n",
    "<b>Categorical features:</b><br>\n",
    "1. Area code\n",
    "2. Internation Plan\n",
    "3. Voice Mail Plan\n",
    "\n",
    "<b>Numerical features:</b><br>\n",
    "1. number_vmail_messages<br>\n",
    "2. total_day_minutes<br>\n",
    "3. total_day_calls<br>\n",
    "4. total_day_charge<br>\n",
    "5. total_eve_minutes<br>\n",
    "6. total_eve_calls<br>\n",
    "7. total_eve_charge<br>\n",
    "8. total_night_minutes<br>\n",
    "9. total_night_calls<br>\n",
    "10. total_night_charge<br>\n",
    "11. total_intl_minutes<br>\n",
    "12. total_intl_calls<br>\n",
    "13. total_intl_charge<br>\n",
    "14. number_customer_service_calls<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c84e1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502e2c2",
   "metadata": {},
   "source": [
    "### 4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7736e0",
   "metadata": {},
   "source": [
    "#### 4.1 Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed748d82",
   "metadata": {},
   "source": [
    "<h4>Lets split the data into train & test. Later build model and evaluate it by performing hyperparameter tuning.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c99811ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"processed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5c6fc338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['area_code' 'international_plan' 'voice_mail_plan'\n",
      " 'number_vmail_messages' 'total_day_minutes' 'total_day_calls'\n",
      " 'total_day_charge' 'total_eve_minutes' 'total_eve_calls'\n",
      " 'total_eve_charge' 'total_night_minutes' 'total_night_calls'\n",
      " 'total_night_charge' 'total_intl_minutes' 'total_intl_calls'\n",
      " 'total_intl_charge' 'number_customer_service_calls' 'churn']\n"
     ]
    }
   ],
   "source": [
    "print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c55d116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4250, 18)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4f8e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (2975, 17)\n",
      "X_test shape (1275, 17)\n",
      "y_train shape (2975,)\n",
      "y_train shape (1275,)\n"
     ]
    }
   ],
   "source": [
    "X = train.drop(['churn'], axis=1)\n",
    "y = train['churn'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"y_train shape\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2db1c",
   "metadata": {},
   "source": [
    "#### 4.2 Vectorize data\n",
    "We will also maintain a list of objects and later we will store them in pickle file. This will be useful when we deploy our model when a new query point comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ac1b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0853198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vectorizers = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b992b",
   "metadata": {},
   "source": [
    "##### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff1aa344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After one hot encoding 'area_code'\n",
      "X_train shape:  (2975, 3)\n",
      "X_test shape:  (1275, 3)\n"
     ]
    }
   ],
   "source": [
    "# area_code\n",
    "areacode_vectorizer = CountVectorizer()\n",
    "areacode_vectorizer.fit(X_train['area_code'].values)\n",
    "X_train_areacode_vectorized = areacode_vectorizer.transform(X_train['area_code'].values)\n",
    "X_test_areacode_vectorized = areacode_vectorizer.transform(X_test['area_code'].values)\n",
    "print(\"\\nAfter one hot encoding 'area_code'\")\n",
    "print(\"X_train shape: \", X_train_areacode_vectorized.shape)\n",
    "print(\"X_test shape: \", X_test_areacode_vectorized.shape)\n",
    "categorical_vectorizers['areacode_vectorizer'] = areacode_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54028c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical vectorizers: {'areacode_vectorizer': CountVectorizer()}\n"
     ]
    }
   ],
   "source": [
    "print('Categorical vectorizers: {}'.format(categorical_vectorizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.extend(areacode_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4febec",
   "metadata": {},
   "source": [
    "##### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ebe2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_vectorizers = {}\n",
    "scaler = MinMaxScaler()\n",
    "def vectorizeNumericalFeature(X_train, X_test, feature):\n",
    "    scaler.fit(X_train[feature].values.reshape(-1,1))\n",
    "    X_train_vectorized_feature = scaler.transform(X_train[feature].values.reshape(-1,1))\n",
    "    X_test_vectorized_feature = scaler.transform(X_test[feature].values.reshape(-1,1))\n",
    "    print(\"\\nAfter normalizing \",feature)\n",
    "    print(\"X_train shape: \", X_train_vectorized_feature.shape)\n",
    "    print(\"X_test shape: \", X_test_vectorized_feature.shape)\n",
    "    my_vect = feature+\"_vectorizer\"\n",
    "    numeric_vectorizers[my_vect] = scaler\n",
    "    feature_names.append(feature)\n",
    "    return X_train_vectorized_feature, X_test_vectorized_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c01a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After normalizing  number_vmail_messages\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_day_minutes\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_day_calls\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_day_charge\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_eve_minutes\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_eve_calls\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_eve_charge\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_night_minutes\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_night_calls\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_night_charge\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_intl_minutes\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_intl_calls\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  total_intl_charge\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n",
      "\n",
      "After normalizing  number_customer_service_calls\n",
      "X_train shape:  (2975, 1)\n",
      "X_test shape:  (1275, 1)\n"
     ]
    }
   ],
   "source": [
    "# number_vmail_messages\n",
    "X_train_numvmailmsg_norm, X_test_numvmailmsg_norm = vectorizeNumericalFeature(X_train, X_test, 'number_vmail_messages')\n",
    "\n",
    "# total_day_minutes\n",
    "X_train_totdaymins_norm, X_test_totdaymins_norm = vectorizeNumericalFeature(X_train, X_test, 'total_day_minutes')\n",
    "\n",
    "# total_day_calls\n",
    "X_train_totdaycalls_norm, X_test_totdaycalls_norm = vectorizeNumericalFeature(X_train, X_test, 'total_day_calls')\n",
    "\n",
    "# total_day_charge\n",
    "X_train_totdaycharge_norm, X_test_totdaycharge_norm = vectorizeNumericalFeature(X_train, X_test, 'total_day_charge')\n",
    "\n",
    "# total_eve_minutes\n",
    "X_train_totevemins_norm, X_test_totevemins_norm = vectorizeNumericalFeature(X_train, X_test, 'total_eve_minutes')\n",
    "\n",
    "# total_eve_calls\n",
    "X_train_totevecalls_norm, X_test_totevecalls_norm = vectorizeNumericalFeature(X_train, X_test, 'total_eve_calls')\n",
    "\n",
    "# total_eve_charge\n",
    "X_train_totevecharge_norm, X_test_totevecharge_norm = vectorizeNumericalFeature(X_train, X_test, 'total_eve_charge')\n",
    "\n",
    "# total_night_minutes\n",
    "X_train_totnightmins_norm, X_test_totnightmins_norm = vectorizeNumericalFeature(X_train, X_test, 'total_night_minutes')\n",
    "\n",
    "# total_night_calls\n",
    "X_train_totnightcalls_norm, X_test_totnightcalls_norm = vectorizeNumericalFeature(X_train, X_test, 'total_night_calls')\n",
    "\n",
    "# total_night_charge\n",
    "X_train_totnightcharge_norm, X_test_totnightcharge_norm = vectorizeNumericalFeature(X_train, X_test, 'total_night_charge')\n",
    "\n",
    "# total_intl_minutes\n",
    "X_train_totintlmins_norm, X_test_totintlmins_norm = vectorizeNumericalFeature(X_train, X_test, 'total_intl_minutes')\n",
    "\n",
    "# total_intl_calls\n",
    "X_train_totintlcalls_norm, X_test_totintlcalls_norm = vectorizeNumericalFeature(X_train, X_test, 'total_intl_calls')\n",
    "\n",
    "# total_intl_charge\n",
    "X_train_totintlcharge_norm, X_test_totintlcharge_norm = vectorizeNumericalFeature(X_train, X_test, 'total_intl_charge')\n",
    "\n",
    "# number_customer_service_calls\n",
    "X_train_custservcalls_norm, X_test_custservcalls_norm = vectorizeNumericalFeature(X_train, X_test, 'number_customer_service_calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed5dd2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical vectorizers: {'number_vmail_messages_vectorizer': MinMaxScaler(), 'total_day_minutes_vectorizer': MinMaxScaler(), 'total_day_calls_vectorizer': MinMaxScaler(), 'total_day_charge_vectorizer': MinMaxScaler(), 'total_eve_minutes_vectorizer': MinMaxScaler(), 'total_eve_calls_vectorizer': MinMaxScaler(), 'total_eve_charge_vectorizer': MinMaxScaler(), 'total_night_minutes_vectorizer': MinMaxScaler(), 'total_night_calls_vectorizer': MinMaxScaler(), 'total_night_charge_vectorizer': MinMaxScaler(), 'total_intl_minutes_vectorizer': MinMaxScaler(), 'total_intl_calls_vectorizer': MinMaxScaler(), 'total_intl_charge_vectorizer': MinMaxScaler(), 'number_customer_service_calls_vectorizer': MinMaxScaler()}\n"
     ]
    }
   ],
   "source": [
    "print('Numerical vectorizers: {}'.format(numeric_vectorizers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac8420",
   "metadata": {},
   "source": [
    "#### Reshape the features which were already in 0/1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d8ca02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "X_train_intplan_ohe = X_train['international_plan'].values.reshape(-1,1)\n",
    "X_test_intplan_ohe = X_test['international_plan'].values.reshape(-1,1)\n",
    "X_train_vmailplan_ohe = X_train['voice_mail_plan'].values.reshape(-1,1)\n",
    "X_test_vmailplan_ohe = X_test['voice_mail_plan'].values.reshape(-1,1)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee4c81",
   "metadata": {},
   "source": [
    "<h4>Lets stack the vectorized features using hstack and create 2 sets (train & test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9fbe697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked data set: \n",
      "X_train shape:  (2975, 19)\n",
      "X_test shape:  (1275, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train_stacked = hstack((X_train_areacode_vectorized, X_train_intplan_ohe, X_train_vmailplan_ohe, X_train_numvmailmsg_norm, X_train_totdaymins_norm, X_train_totdaycalls_norm, X_train_totdaycharge_norm, X_train_totevemins_norm, X_train_totevecalls_norm, X_train_totevecharge_norm, X_train_totnightmins_norm, X_train_totnightcalls_norm, X_train_totnightcharge_norm, X_train_totintlmins_norm, X_train_totintlcalls_norm, X_train_totintlcharge_norm, X_train_custservcalls_norm)).tocsr()\n",
    "X_test_stacked = hstack((X_test_areacode_vectorized, X_test_intplan_ohe, X_test_vmailplan_ohe, X_test_numvmailmsg_norm, X_test_totdaymins_norm, X_test_totdaycalls_norm, X_test_totdaycharge_norm, X_test_totevemins_norm, X_test_totevecalls_norm, X_test_totevecharge_norm, X_test_totnightmins_norm, X_test_totnightcalls_norm, X_test_totnightcharge_norm, X_test_totintlmins_norm, X_test_totintlcalls_norm, X_test_totintlcharge_norm, X_test_custservcalls_norm)).tocsr()\n",
    "\n",
    "print(\"Stacked data set: \")\n",
    "print(\"X_train shape: \", X_train_stacked.shape)\n",
    "print(\"X_test shape: \", X_test_stacked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3b0ff",
   "metadata": {},
   "source": [
    "Import some common ML functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec8cf743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6030e4c",
   "metadata": {},
   "source": [
    "### 5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34d5f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d902e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression() started...\n",
      "LogisticRegression() completed... Time taken: 0:00:00.027923\n",
      "KNeighborsClassifier() started...\n",
      "KNeighborsClassifier() completed... Time taken: 0:00:00.210118\n",
      "RandomForestClassifier() started...\n",
      "RandomForestClassifier() completed... Time taken: 0:00:01.151270\n",
      "DecisionTreeClassifier() started...\n",
      "DecisionTreeClassifier() completed... Time taken: 0:00:00.052666\n",
      "GradientBoostingClassifier() started...\n",
      "GradientBoostingClassifier() completed... Time taken: 0:00:00.815453\n",
      "SVC() started...\n",
      "SVC() completed... Time taken: 0:00:00.178173\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for classifier in [LogisticRegression(), KNeighborsClassifier(), RandomForestClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), SVC()]:\n",
    "    start = datetime.now()\n",
    "    clf = classifier\n",
    "    clf_str = str(clf).split(' ')[0].split('.')[-1]\n",
    "    print('{} started...'.format(clf_str))\n",
    "    clf.fit(X_train_stacked, y_train)\n",
    "    y_pred = clf.predict(X_test_stacked)\n",
    "    print('{} completed... Time taken: {}'.format(clf_str, datetime.now()-start))\n",
    "    \n",
    "    temp = list()\n",
    "    temp.append(clf_str)\n",
    "    temp.append('Default')\n",
    "    temp.append(accuracy_score(y_test, y_pred))\n",
    "    temp.append(confusion_matrix(y_test, y_pred))\n",
    "    temp.append(datetime.now()-start)\n",
    "    results.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f9e85e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Time taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>Default</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>[[1084, 12], [163, 16]]</td>\n",
       "      <td>0 days 00:00:00.028921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC()</td>\n",
       "      <td>Default</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>[[1096, 0], [170, 9]]</td>\n",
       "      <td>0 days 00:00:00.179173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>Default</td>\n",
       "      <td>0.876863</td>\n",
       "      <td>[[1001, 95], [62, 117]]</td>\n",
       "      <td>0 days 00:00:00.053667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>Default</td>\n",
       "      <td>0.883922</td>\n",
       "      <td>[[1086, 10], [138, 41]]</td>\n",
       "      <td>0 days 00:00:00.211135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>Default</td>\n",
       "      <td>0.931765</td>\n",
       "      <td>[[1081, 15], [72, 107]]</td>\n",
       "      <td>0 days 00:00:00.816420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>Default</td>\n",
       "      <td>0.937255</td>\n",
       "      <td>[[1087, 9], [71, 108]]</td>\n",
       "      <td>0 days 00:00:01.152272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Algorithm Hyperparameters  Accuracy  \\\n",
       "0          LogisticRegression()         Default  0.862745   \n",
       "5                         SVC()         Default  0.866667   \n",
       "3      DecisionTreeClassifier()         Default  0.876863   \n",
       "1        KNeighborsClassifier()         Default  0.883922   \n",
       "4  GradientBoostingClassifier()         Default  0.931765   \n",
       "2      RandomForestClassifier()         Default  0.937255   \n",
       "\n",
       "          Confusion Matrix             Time taken  \n",
       "0  [[1084, 12], [163, 16]] 0 days 00:00:00.028921  \n",
       "5    [[1096, 0], [170, 9]] 0 days 00:00:00.179173  \n",
       "3  [[1001, 95], [62, 117]] 0 days 00:00:00.053667  \n",
       "1  [[1086, 10], [138, 41]] 0 days 00:00:00.211135  \n",
       "4  [[1081, 15], [72, 107]] 0 days 00:00:00.816420  \n",
       "2   [[1087, 9], [71, 108]] 0 days 00:00:01.152272  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, columns=['Algorithm', 'Hyperparameters', 'Accuracy', 'Confusion Matrix', 'Time taken']).sort_values('Accuracy')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18612e",
   "metadata": {},
   "source": [
    "#### GradientBoosting has given us 95.68% accuracy. Lets tune this to see if we get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55f69714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c34304b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV started...\n",
      "GridSearchCV completed... Time taken: 0:12:43.422739\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print('GridSearchCV started...')\n",
    "parameters = {'max_depth':(1, 3, 10, 30), 'min_samples_split':(5, 10, 100, 500), 'n_estimators':(50,100,200)}\n",
    "gbdt = GradientBoostingClassifier()\n",
    "clf = GridSearchCV(gbdt, parameters, return_train_score=True, scoring='accuracy', cv=5)\n",
    "clf.fit(X_train_stacked, y_train)\n",
    "print('GridSearchCV completed... Time taken: {}'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "296dd0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = pd.DataFrame.from_dict(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "19ddcf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.775754</td>\n",
       "      <td>0.653045</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 30, 'min_samples_split': 100, 'n...</td>\n",
       "      <td>0.909244</td>\n",
       "      <td>0.922689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924034</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.212015</td>\n",
       "      <td>0.355322</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 30, 'min_samples_split': 100, 'n...</td>\n",
       "      <td>0.912605</td>\n",
       "      <td>0.922689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923361</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.694178</td>\n",
       "      <td>0.283163</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>30</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 30, 'min_samples_split': 500, 'n...</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.921008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922689</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>3</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>0.99916</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>0.998739</td>\n",
       "      <td>0.998739</td>\n",
       "      <td>0.999160</td>\n",
       "      <td>0.000376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "43       5.775754      0.653045         0.011317        0.000578   \n",
       "42       2.212015      0.355322         0.004808        0.000764   \n",
       "47       5.694178      0.283163         0.015105        0.001222   \n",
       "\n",
       "   param_max_depth param_min_samples_split param_n_estimators  \\\n",
       "43              30                     100                100   \n",
       "42              30                     100                 50   \n",
       "47              30                     500                200   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "43  {'max_depth': 30, 'min_samples_split': 100, 'n...           0.909244   \n",
       "42  {'max_depth': 30, 'min_samples_split': 100, 'n...           0.912605   \n",
       "47  {'max_depth': 30, 'min_samples_split': 500, 'n...           0.917647   \n",
       "\n",
       "    split1_test_score  ...  mean_test_score  std_test_score  rank_test_score  \\\n",
       "43           0.922689  ...         0.924034        0.008329                1   \n",
       "42           0.922689  ...         0.923361        0.006252                2   \n",
       "47           0.921008  ...         0.922689        0.004986                3   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "43             1.00000             1.00000             1.00000   \n",
       "42             1.00000             0.99958             0.99958   \n",
       "47             0.99958             0.99916             0.99958   \n",
       "\n",
       "    split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "43            1.000000            1.000000          1.000000         0.000000  \n",
       "42            0.999580            1.000000          0.999748         0.000206  \n",
       "47            0.998739            0.998739          0.999160         0.000376  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.sort_values(by='mean_test_score', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a212ac",
   "metadata": {},
   "source": [
    "We will take the bets parameters that we got through cross validation and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e5dbe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting with parameters: 'max_depth': 30, 'min_samples_split': 100, 'n_estimators':100\n",
      "\n",
      "Training completed... Time taken: 0:00:05.427328\n",
      "Accuracy: 0.9317647058823529\n",
      "Confusion matrix: \n",
      "[[1078   18]\n",
      " [  69  110]]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Training Gradient Boosting with parameters: 'max_depth': 30, 'min_samples_split': 100, 'n_estimators':100\\n\")\n",
    "clf = GradientBoostingClassifier(max_depth=30, min_samples_split=100, n_estimators=100)\n",
    "clf.fit(X_train_stacked, y_train)\n",
    "print(\"Training completed... Time taken: {}\".format(datetime.now()-start))\n",
    "y_pred = clf.predict(X_test_stacked)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Confusion matrix: \\n{}'.format(confusion_matrix(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af62b20",
   "metadata": {},
   "source": [
    "Lets check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3ccb523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(data=zip(feature_names, list(clf.feature_importances_)), columns=['feature', 'importance']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16dd3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total_day_minutes</td>\n",
       "      <td>0.204241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>total_day_charge</td>\n",
       "      <td>0.149033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total_eve_minutes</td>\n",
       "      <td>0.114907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>number_customer_service_calls</td>\n",
       "      <td>0.113268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>total_eve_charge</td>\n",
       "      <td>0.093504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>number_vmail_messages</td>\n",
       "      <td>0.079299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>total_night_minutes</td>\n",
       "      <td>0.039573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>total_night_calls</td>\n",
       "      <td>0.032813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>total_intl_charge</td>\n",
       "      <td>0.031660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>total_day_calls</td>\n",
       "      <td>0.030921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_intl_minutes</td>\n",
       "      <td>0.029620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>total_intl_calls</td>\n",
       "      <td>0.026362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>total_night_charge</td>\n",
       "      <td>0.026306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>total_eve_calls</td>\n",
       "      <td>0.023329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voice_mail_plan</td>\n",
       "      <td>0.002198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>area_code_510</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area_code_415</td>\n",
       "      <td>0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area_code_408</td>\n",
       "      <td>0.000620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>international_plan</td>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          feature  importance\n",
       "6               total_day_minutes    0.204241\n",
       "8                total_day_charge    0.149033\n",
       "9               total_eve_minutes    0.114907\n",
       "18  number_customer_service_calls    0.113268\n",
       "11               total_eve_charge    0.093504\n",
       "5           number_vmail_messages    0.079299\n",
       "12            total_night_minutes    0.039573\n",
       "13              total_night_calls    0.032813\n",
       "17              total_intl_charge    0.031660\n",
       "7                 total_day_calls    0.030921\n",
       "15             total_intl_minutes    0.029620\n",
       "16               total_intl_calls    0.026362\n",
       "14             total_night_charge    0.026306\n",
       "10                total_eve_calls    0.023329\n",
       "4                 voice_mail_plan    0.002198\n",
       "2                   area_code_510    0.000930\n",
       "1                   area_code_415    0.000877\n",
       "0                   area_code_408    0.000620\n",
       "3              international_plan    0.000539"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c765bd5",
   "metadata": {},
   "source": [
    "Now, our plan is to deploy this model to production and expose it through API. We will provide a web interface where user can input their telco usage and our model will predict whether the customer is likely to churn or not.\n",
    "<br>\n",
    "The process will be as follows:\n",
    "1. User will enter all the required features using web interface\n",
    "2. We will take the input as query point\n",
    "3. The query point will go through preprocessing steps\n",
    "4. After preprocessing, we will vectorize the query point\n",
    "5. After vectorization, we will provide this as input to our GBDT model\n",
    "6. Based on the result, we will update the on the web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc83d3f",
   "metadata": {},
   "source": [
    "<b>Preprocessing steps to do:</b>\n",
    "1. For 'international_plan': if 'yes' then 1 else 0\n",
    "2. For 'voice_mail_plan': : if 'yes' then 1 else 0\n",
    "3. Drop 'state' & 'account_length'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771f485",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "We will run the query points through below vectorizers. We will be dumping all these vectorizers to pickle file now along with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b2515d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'areacode_vectorizer': CountVectorizer()}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3bbed138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number_vmail_messages_vectorizer': MinMaxScaler(),\n",
       " 'total_day_minutes_vectorizer': MinMaxScaler(),\n",
       " 'total_day_calls_vectorizer': MinMaxScaler(),\n",
       " 'total_day_charge_vectorizer': MinMaxScaler(),\n",
       " 'total_eve_minutes_vectorizer': MinMaxScaler(),\n",
       " 'total_eve_calls_vectorizer': MinMaxScaler(),\n",
       " 'total_eve_charge_vectorizer': MinMaxScaler(),\n",
       " 'total_night_minutes_vectorizer': MinMaxScaler(),\n",
       " 'total_night_calls_vectorizer': MinMaxScaler(),\n",
       " 'total_night_charge_vectorizer': MinMaxScaler(),\n",
       " 'total_intl_minutes_vectorizer': MinMaxScaler(),\n",
       " 'total_intl_calls_vectorizer': MinMaxScaler(),\n",
       " 'total_intl_charge_vectorizer': MinMaxScaler(),\n",
       " 'number_customer_service_calls_vectorizer': MinMaxScaler()}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "88a52018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped: areacode_vectorizer\n",
      "Dumped: number_vmail_messages_vectorizer\n",
      "Dumped: total_day_minutes_vectorizer\n",
      "Dumped: total_day_calls_vectorizer\n",
      "Dumped: total_day_charge_vectorizer\n",
      "Dumped: total_eve_minutes_vectorizer\n",
      "Dumped: total_eve_calls_vectorizer\n",
      "Dumped: total_eve_charge_vectorizer\n",
      "Dumped: total_night_minutes_vectorizer\n",
      "Dumped: total_night_calls_vectorizer\n",
      "Dumped: total_night_charge_vectorizer\n",
      "Dumped: total_intl_minutes_vectorizer\n",
      "Dumped: total_intl_calls_vectorizer\n",
      "Dumped: total_intl_charge_vectorizer\n",
      "Dumped: number_customer_service_calls_vectorizer\n",
      "Dumped: GradientBoostingClassifier(max_depth=30, min_samples_split=100)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('gbdt.pkl','wb')\n",
    "for vec in categorical_vectorizers:\n",
    "    pickle.dump(categorical_vectorizers[vec], file)\n",
    "    print('Dumped: {}'.format(vec))\n",
    "\n",
    "for vec in numeric_vectorizers:\n",
    "    pickle.dump(numeric_vectorizers[vec], file)\n",
    "    print('Dumped: {}'.format(vec))\n",
    "    \n",
    "pickle.dump(clf, file)\n",
    "print('Dumped: {}'.format(clf))\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds] *",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
